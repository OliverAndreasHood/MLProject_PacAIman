{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T00:30:14.714423Z",
     "iopub.status.busy": "2022-01-26T00:30:14.714423Z",
     "iopub.status.idle": "2022-01-26T00:30:14.724408Z",
     "shell.execute_reply": "2022-01-26T00:30:14.724408Z",
     "shell.execute_reply.started": "2022-01-26T00:30:14.714423Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install tensorflow==2.3.1 gym keras-rl2 gym[atari]\n",
    "#%pip install gym[accept-rom-license]\n",
    "#%pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:27:52.451410Z",
     "iopub.status.busy": "2022-01-26T09:27:52.451410Z",
     "iopub.status.idle": "2022-01-26T09:27:52.750678Z",
     "shell.execute_reply": "2022-01-26T09:27:52.750678Z",
     "shell.execute_reply.started": "2022-01-26T09:27:52.451410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:27:52.751680Z",
     "iopub.status.busy": "2022-01-26T09:27:52.751680Z",
     "iopub.status.idle": "2022-01-26T09:27:52.829749Z",
     "shell.execute_reply": "2022-01-26T09:27:52.829749Z",
     "shell.execute_reply.started": "2022-01-26T09:27:52.751680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('MsPacman-v0')\n",
    "#env = gym.make('ALE/MsPacman-v5')#, render_mode='human')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "# sprawdzić evny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:27:55.444244Z",
     "iopub.status.busy": "2022-01-26T09:27:55.444244Z",
     "iopub.status.idle": "2022-01-26T09:27:55.456255Z",
     "shell.execute_reply": "2022-01-26T09:27:55.456255Z",
     "shell.execute_reply.started": "2022-01-26T09:27:55.444244Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'DOWNRIGHT',\n",
       " 'DOWNLEFT']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:27:58.457710Z",
     "iopub.status.busy": "2022-01-26T09:27:58.457710Z",
     "iopub.status.idle": "2022-01-26T09:28:01.518060Z",
     "shell.execute_reply": "2022-01-26T09:28:01.518060Z",
     "shell.execute_reply.started": "2022-01-26T09:27:58.457710Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milod\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\milod\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Po ustawieniu trybu wątku nie można go zmienić\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:250.0\n",
      "Episode:2 Score:210.0\n",
      "Episode:3 Score:240.0\n",
      "Episode:4 Score:170.0\n",
      "Episode:5 Score:340.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice(range(actions))\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:05.801385Z",
     "iopub.status.busy": "2022-01-26T09:28:05.801385Z",
     "iopub.status.idle": "2022-01-26T09:28:05.816399Z",
     "shell.execute_reply": "2022-01-26T09:28:05.816399Z",
     "shell.execute_reply.started": "2022-01-26T09:28:05.801385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:05.817399Z",
     "iopub.status.busy": "2022-01-26T09:28:05.817399Z",
     "iopub.status.idle": "2022-01-26T09:28:05.832413Z",
     "shell.execute_reply": "2022-01-26T09:28:05.832413Z",
     "shell.execute_reply.started": "2022-01-26T09:28:05.817399Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential(layers=[\n",
    "        Convolution2D(32, (8, 8), strides = (4,4), activation='relu', input_shape = (3,height,width,channels)),\n",
    "        Convolution2D(64, (4, 4), strides = (2,2), activation='relu'),\n",
    "        Convolution2D(64, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(actions, activation='linear')])\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:08.016412Z",
     "iopub.status.busy": "2022-01-26T09:28:08.016412Z",
     "iopub.status.idle": "2022-01-26T09:28:08.030425Z",
     "shell.execute_reply": "2022-01-26T09:28:08.030425Z",
     "shell.execute_reply.started": "2022-01-26T09:28:08.016412Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:10.433410Z",
     "iopub.status.busy": "2022-01-26T09:28:10.433410Z",
     "iopub.status.idle": "2022-01-26T09:28:11.034950Z",
     "shell.execute_reply": "2022-01-26T09:28:11.034950Z",
     "shell.execute_reply.started": "2022-01-26T09:28:10.433410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 34,813,097\n",
      "Trainable params: 34,813,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:11.035951Z",
     "iopub.status.busy": "2022-01-26T09:28:11.035951Z",
     "iopub.status.idle": "2022-01-26T09:28:11.050964Z",
     "shell.execute_reply": "2022-01-26T09:28:11.050964Z",
     "shell.execute_reply.started": "2022-01-26T09:28:11.035951Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:13.454205Z",
     "iopub.status.busy": "2022-01-26T09:28:13.454205Z",
     "iopub.status.idle": "2022-01-26T09:28:13.467217Z",
     "shell.execute_reply": "2022-01-26T09:28:13.467217Z",
     "shell.execute_reply.started": "2022-01-26T09:28:13.454205Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=100000, window_length=3) #limit = 100 000\n",
    "    dqn = DQNAgent(\n",
    "        model=model, \n",
    "        memory=memory, \n",
    "        policy=policy, \n",
    "        enable_dueling_network=True, \n",
    "        dueling_type='avg', \n",
    "        nb_actions=actions,\n",
    "        nb_steps_warmup=1000\n",
    "    )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:15.800409Z",
     "iopub.status.busy": "2022-01-26T09:28:15.800409Z",
     "iopub.status.idle": "2022-01-26T09:28:15.877478Z",
     "shell.execute_reply": "2022-01-26T09:28:15.877478Z",
     "shell.execute_reply.started": "2022-01-26T09:28:15.800409Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 34,813,097\n",
      "Trainable params: 34,813,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:19.439416Z",
     "iopub.status.busy": "2022-01-26T09:28:19.439416Z",
     "iopub.status.idle": "2022-01-26T09:28:19.961881Z",
     "shell.execute_reply": "2022-01-26T09:28:19.961881Z",
     "shell.execute_reply.started": "2022-01-26T09:28:19.439416Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:22.436414Z",
     "iopub.status.busy": "2022-01-26T09:28:22.436414Z",
     "iopub.status.idle": "2022-01-26T09:28:22.451428Z",
     "shell.execute_reply": "2022-01-26T09:28:22.451428Z",
     "shell.execute_reply.started": "2022-01-26T09:28:22.436414Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:25.443412Z",
     "iopub.status.busy": "2022-01-26T09:28:25.443412Z",
     "iopub.status.idle": "2022-01-26T09:28:25.455423Z",
     "shell.execute_reply": "2022-01-26T09:28:25.455423Z",
     "shell.execute_reply.started": "2022-01-26T09:28:25.443412Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T00:30:21.099405Z",
     "iopub.status.busy": "2022-01-26T00:30:21.099405Z",
     "iopub.status.idle": "2022-01-26T07:30:27.703409Z",
     "shell.execute_reply": "2022-01-26T07:30:27.702408Z",
     "shell.execute_reply.started": "2022-01-26T00:30:21.099405Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milod\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   624/50000: episode: 1, duration: 7.534s, episode steps: 624, steps per second:  83, episode reward: 210.000, mean reward:  0.337 [ 0.000, 10.000], mean action: 3.870 [0.000, 8.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milod\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1274/50000: episode: 2, duration: 145.707s, episode steps: 650, steps per second:   4, episode reward: 190.000, mean reward:  0.292 [ 0.000, 10.000], mean action: 4.080 [0.000, 8.000],  loss: 54.115806, mean_q: 16.609144, mean_eps: 0.897670\n",
      "  2336/50000: episode: 3, duration: 548.229s, episode steps: 1062, steps per second:   2, episode reward: 600.000, mean reward:  0.565 [ 0.000, 200.000], mean action: 4.217 [0.000, 8.000],  loss: 4.375737, mean_q: 15.213529, mean_eps: 0.837595\n",
      "  2882/50000: episode: 4, duration: 284.823s, episode steps: 546, steps per second:   2, episode reward: 150.000, mean reward:  0.275 [ 0.000, 10.000], mean action: 3.813 [0.000, 8.000],  loss: 8.238887, mean_q: 15.116696, mean_eps: 0.765235\n",
      "  3612/50000: episode: 5, duration: 377.314s, episode steps: 730, steps per second:   2, episode reward: 400.000, mean reward:  0.548 [ 0.000, 10.000], mean action: 4.273 [0.000, 8.000],  loss: 3.364704, mean_q: 15.662605, mean_eps: 0.707815\n",
      "  4308/50000: episode: 6, duration: 363.120s, episode steps: 696, steps per second:   2, episode reward: 220.000, mean reward:  0.316 [ 0.000, 10.000], mean action: 4.080 [0.000, 8.000],  loss: 0.676044, mean_q: 15.127507, mean_eps: 0.643645\n",
      "  5336/50000: episode: 7, duration: 531.319s, episode steps: 1028, steps per second:   2, episode reward: 640.000, mean reward:  0.623 [ 0.000, 200.000], mean action: 4.167 [0.000, 8.000],  loss: 1.605529, mean_q: 14.931732, mean_eps: 0.566065\n",
      "  6030/50000: episode: 8, duration: 354.195s, episode steps: 694, steps per second:   2, episode reward: 250.000, mean reward:  0.360 [ 0.000, 10.000], mean action: 4.052 [0.000, 8.000],  loss: 1.775571, mean_q: 14.602756, mean_eps: 0.488575\n",
      "  6590/50000: episode: 9, duration: 285.817s, episode steps: 560, steps per second:   2, episode reward: 210.000, mean reward:  0.375 [ 0.000, 10.000], mean action: 3.654 [0.000, 8.000],  loss: 3.097478, mean_q: 14.629366, mean_eps: 0.432145\n",
      "  7456/50000: episode: 10, duration: 444.507s, episode steps: 866, steps per second:   2, episode reward: 300.000, mean reward:  0.346 [ 0.000, 10.000], mean action: 3.976 [0.000, 8.000],  loss: 3.743960, mean_q: 14.714611, mean_eps: 0.367975\n",
      "  8206/50000: episode: 11, duration: 382.155s, episode steps: 750, steps per second:   2, episode reward: 400.000, mean reward:  0.533 [ 0.000, 10.000], mean action: 4.359 [0.000, 8.000],  loss: 4.983234, mean_q: 14.786737, mean_eps: 0.295255\n",
      "  8816/50000: episode: 12, duration: 311.302s, episode steps: 610, steps per second:   2, episode reward: 240.000, mean reward:  0.393 [ 0.000, 10.000], mean action: 3.692 [0.000, 8.000],  loss: 1.513514, mean_q: 14.519408, mean_eps: 0.234055\n",
      "  9464/50000: episode: 13, duration: 330.343s, episode steps: 648, steps per second:   2, episode reward: 380.000, mean reward:  0.586 [ 0.000, 10.000], mean action: 3.657 [0.000, 8.000],  loss: 1.857669, mean_q: 14.641519, mean_eps: 0.177445\n",
      " 10411/50000: episode: 14, duration: 482.467s, episode steps: 947, steps per second:   2, episode reward: 610.000, mean reward:  0.644 [ 0.000, 50.000], mean action: 4.027 [0.000, 8.000],  loss: 4.795047, mean_q: 14.850212, mean_eps: 0.113677\n",
      " 11136/50000: episode: 15, duration: 369.161s, episode steps: 725, steps per second:   2, episode reward: 430.000, mean reward:  0.593 [ 0.000, 10.000], mean action: 4.218 [0.000, 8.000],  loss: 2.918430, mean_q: 14.947978, mean_eps: 0.100000\n",
      " 11638/50000: episode: 16, duration: 255.934s, episode steps: 502, steps per second:   2, episode reward: 150.000, mean reward:  0.299 [ 0.000, 10.000], mean action: 3.803 [0.000, 8.000],  loss: 1.467593, mean_q: 14.818887, mean_eps: 0.100000\n",
      " 12115/50000: episode: 17, duration: 243.157s, episode steps: 477, steps per second:   2, episode reward: 290.000, mean reward:  0.608 [ 0.000, 10.000], mean action: 4.512 [0.000, 8.000],  loss: 1.567460, mean_q: 14.696386, mean_eps: 0.100000\n",
      " 12842/50000: episode: 18, duration: 370.686s, episode steps: 727, steps per second:   2, episode reward: 250.000, mean reward:  0.344 [ 0.000, 10.000], mean action: 3.997 [0.000, 8.000],  loss: 1.514198, mean_q: 14.824341, mean_eps: 0.100000\n",
      " 14340/50000: episode: 19, duration: 762.960s, episode steps: 1498, steps per second:   2, episode reward: 920.000, mean reward:  0.614 [ 0.000, 200.000], mean action: 3.513 [0.000, 8.000],  loss: 2.107379, mean_q: 14.743042, mean_eps: 0.100000\n",
      " 14936/50000: episode: 20, duration: 303.737s, episode steps: 596, steps per second:   2, episode reward: 390.000, mean reward:  0.654 [ 0.000, 10.000], mean action: 3.436 [0.000, 8.000],  loss: 2.454695, mean_q: 14.770061, mean_eps: 0.100000\n",
      " 15627/50000: episode: 21, duration: 352.216s, episode steps: 691, steps per second:   2, episode reward: 270.000, mean reward:  0.391 [ 0.000, 10.000], mean action: 4.072 [0.000, 8.000],  loss: 1.443784, mean_q: 14.653329, mean_eps: 0.100000\n",
      " 16264/50000: episode: 22, duration: 324.917s, episode steps: 637, steps per second:   2, episode reward: 210.000, mean reward:  0.330 [ 0.000, 10.000], mean action: 3.684 [0.000, 8.000],  loss: 1.634719, mean_q: 14.579331, mean_eps: 0.100000\n",
      " 16967/50000: episode: 23, duration: 358.259s, episode steps: 703, steps per second:   2, episode reward: 410.000, mean reward:  0.583 [ 0.000, 10.000], mean action: 3.464 [0.000, 8.000],  loss: 1.671656, mean_q: 14.675360, mean_eps: 0.100000\n",
      " 17685/50000: episode: 24, duration: 366.038s, episode steps: 718, steps per second:   2, episode reward: 390.000, mean reward:  0.543 [ 0.000, 10.000], mean action: 3.781 [0.000, 8.000],  loss: 1.221803, mean_q: 14.425553, mean_eps: 0.100000\n",
      " 18218/50000: episode: 25, duration: 271.787s, episode steps: 533, steps per second:   2, episode reward: 240.000, mean reward:  0.450 [ 0.000, 10.000], mean action: 4.255 [0.000, 8.000],  loss: 2.054101, mean_q: 14.672345, mean_eps: 0.100000\n",
      " 19496/50000: episode: 26, duration: 651.692s, episode steps: 1278, steps per second:   2, episode reward: 760.000, mean reward:  0.595 [ 0.000, 200.000], mean action: 3.887 [0.000, 8.000],  loss: 1.442239, mean_q: 14.589004, mean_eps: 0.100000\n",
      " 20188/50000: episode: 27, duration: 353.188s, episode steps: 692, steps per second:   2, episode reward: 390.000, mean reward:  0.564 [ 0.000, 10.000], mean action: 3.668 [0.000, 8.000],  loss: 4.841469, mean_q: 15.099800, mean_eps: 0.100000\n",
      " 21309/50000: episode: 28, duration: 572.061s, episode steps: 1121, steps per second:   2, episode reward: 800.000, mean reward:  0.714 [ 0.000, 200.000], mean action: 4.112 [0.000, 8.000],  loss: 7.460498, mean_q: 16.679383, mean_eps: 0.100000\n",
      " 22025/50000: episode: 29, duration: 365.462s, episode steps: 716, steps per second:   2, episode reward: 390.000, mean reward:  0.545 [ 0.000, 10.000], mean action: 3.696 [0.000, 8.000],  loss: 5.188979, mean_q: 16.516001, mean_eps: 0.100000\n",
      " 22718/50000: episode: 30, duration: 354.061s, episode steps: 693, steps per second:   2, episode reward: 400.000, mean reward:  0.577 [ 0.000, 10.000], mean action: 3.290 [0.000, 8.000],  loss: 1.971159, mean_q: 16.397134, mean_eps: 0.100000\n",
      " 23378/50000: episode: 31, duration: 337.185s, episode steps: 660, steps per second:   2, episode reward: 320.000, mean reward:  0.485 [ 0.000, 10.000], mean action: 3.782 [0.000, 8.000],  loss: 2.921350, mean_q: 16.402130, mean_eps: 0.100000\n",
      " 23906/50000: episode: 32, duration: 269.924s, episode steps: 528, steps per second:   2, episode reward: 270.000, mean reward:  0.511 [ 0.000, 10.000], mean action: 3.326 [0.000, 8.000],  loss: 3.796085, mean_q: 16.395013, mean_eps: 0.100000\n",
      " 24583/50000: episode: 33, duration: 346.843s, episode steps: 677, steps per second:   2, episode reward: 320.000, mean reward:  0.473 [ 0.000, 10.000], mean action: 3.672 [0.000, 8.000],  loss: 1.877002, mean_q: 16.373787, mean_eps: 0.100000\n",
      " 25150/50000: episode: 34, duration: 289.932s, episode steps: 567, steps per second:   2, episode reward: 270.000, mean reward:  0.476 [ 0.000, 10.000], mean action: 3.504 [0.000, 8.000],  loss: 4.610431, mean_q: 16.584499, mean_eps: 0.100000\n",
      " 25908/50000: episode: 35, duration: 387.587s, episode steps: 758, steps per second:   2, episode reward: 460.000, mean reward:  0.607 [ 0.000, 10.000], mean action: 3.512 [0.000, 8.000],  loss: 2.464450, mean_q: 16.443081, mean_eps: 0.100000\n",
      " 26847/50000: episode: 36, duration: 480.255s, episode steps: 939, steps per second:   2, episode reward: 880.000, mean reward:  0.937 [ 0.000, 200.000], mean action: 3.936 [0.000, 8.000],  loss: 3.104534, mean_q: 16.567741, mean_eps: 0.100000\n",
      " 27917/50000: episode: 37, duration: 547.506s, episode steps: 1070, steps per second:   2, episode reward: 670.000, mean reward:  0.626 [ 0.000, 200.000], mean action: 3.312 [0.000, 8.000],  loss: 2.519698, mean_q: 16.495168, mean_eps: 0.100000\n",
      " 28611/50000: episode: 38, duration: 355.103s, episode steps: 694, steps per second:   2, episode reward: 390.000, mean reward:  0.562 [ 0.000, 10.000], mean action: 4.246 [0.000, 8.000],  loss: 5.672666, mean_q: 16.637177, mean_eps: 0.100000\n",
      " 29250/50000: episode: 39, duration: 327.591s, episode steps: 639, steps per second:   2, episode reward: 380.000, mean reward:  0.595 [ 0.000, 10.000], mean action: 4.363 [0.000, 8.000],  loss: 3.330086, mean_q: 16.461314, mean_eps: 0.100000\n",
      " 29867/50000: episode: 40, duration: 316.283s, episode steps: 617, steps per second:   2, episode reward: 270.000, mean reward:  0.438 [ 0.000, 10.000], mean action: 3.488 [0.000, 8.000],  loss: 3.556902, mean_q: 16.511816, mean_eps: 0.100000\n",
      " 30472/50000: episode: 41, duration: 310.455s, episode steps: 605, steps per second:   2, episode reward: 290.000, mean reward:  0.479 [ 0.000, 10.000], mean action: 3.850 [0.000, 8.000],  loss: 8.147635, mean_q: 17.888097, mean_eps: 0.100000\n",
      " 31161/50000: episode: 42, duration: 353.856s, episode steps: 689, steps per second:   2, episode reward: 310.000, mean reward:  0.450 [ 0.000, 10.000], mean action: 3.671 [0.000, 8.000],  loss: 3.761125, mean_q: 17.965117, mean_eps: 0.100000\n",
      " 31814/50000: episode: 43, duration: 335.499s, episode steps: 653, steps per second:   2, episode reward: 390.000, mean reward:  0.597 [ 0.000, 10.000], mean action: 3.697 [0.000, 8.000],  loss: 8.168932, mean_q: 18.216126, mean_eps: 0.100000\n",
      " 32730/50000: episode: 44, duration: 470.620s, episode steps: 916, steps per second:   2, episode reward: 460.000, mean reward:  0.502 [ 0.000, 200.000], mean action: 4.199 [0.000, 8.000],  loss: 4.948182, mean_q: 18.010168, mean_eps: 0.100000\n",
      " 33555/50000: episode: 45, duration: 424.264s, episode steps: 825, steps per second:   2, episode reward: 410.000, mean reward:  0.497 [ 0.000, 10.000], mean action: 3.725 [0.000, 8.000],  loss: 2.198384, mean_q: 17.937001, mean_eps: 0.100000\n",
      " 34354/50000: episode: 46, duration: 411.362s, episode steps: 799, steps per second:   2, episode reward: 430.000, mean reward:  0.538 [ 0.000, 10.000], mean action: 3.947 [0.000, 8.000],  loss: 4.320478, mean_q: 18.022916, mean_eps: 0.100000\n",
      " 35456/50000: episode: 47, duration: 567.206s, episode steps: 1102, steps per second:   2, episode reward: 900.000, mean reward:  0.817 [ 0.000, 200.000], mean action: 4.105 [0.000, 8.000],  loss: 3.545668, mean_q: 18.078368, mean_eps: 0.100000\n",
      " 36084/50000: episode: 48, duration: 323.890s, episode steps: 628, steps per second:   2, episode reward: 330.000, mean reward:  0.525 [ 0.000, 10.000], mean action: 3.836 [0.000, 8.000],  loss: 6.255166, mean_q: 18.220495, mean_eps: 0.100000\n",
      " 36649/50000: episode: 49, duration: 291.435s, episode steps: 565, steps per second:   2, episode reward: 350.000, mean reward:  0.619 [ 0.000, 10.000], mean action: 3.782 [0.000, 8.000],  loss: 3.489186, mean_q: 18.216971, mean_eps: 0.100000\n",
      " 37233/50000: episode: 50, duration: 301.078s, episode steps: 584, steps per second:   2, episode reward: 360.000, mean reward:  0.616 [ 0.000, 10.000], mean action: 3.978 [0.000, 8.000],  loss: 2.551851, mean_q: 18.079582, mean_eps: 0.100000\n",
      " 37864/50000: episode: 51, duration: 325.465s, episode steps: 631, steps per second:   2, episode reward: 290.000, mean reward:  0.460 [ 0.000, 10.000], mean action: 3.656 [0.000, 8.000],  loss: 3.527240, mean_q: 18.091604, mean_eps: 0.100000\n",
      " 38419/50000: episode: 52, duration: 288.448s, episode steps: 555, steps per second:   2, episode reward: 200.000, mean reward:  0.360 [ 0.000, 10.000], mean action: 4.135 [0.000, 8.000],  loss: 4.163773, mean_q: 18.067979, mean_eps: 0.100000\n",
      " 39164/50000: episode: 53, duration: 384.895s, episode steps: 745, steps per second:   2, episode reward: 400.000, mean reward:  0.537 [ 0.000, 10.000], mean action: 3.969 [0.000, 8.000],  loss: 2.427627, mean_q: 18.044600, mean_eps: 0.100000\n",
      " 40098/50000: episode: 54, duration: 482.537s, episode steps: 934, steps per second:   2, episode reward: 760.000, mean reward:  0.814 [ 0.000, 200.000], mean action: 3.805 [0.000, 8.000],  loss: 3.102635, mean_q: 18.164298, mean_eps: 0.100000\n",
      " 40833/50000: episode: 55, duration: 379.711s, episode steps: 735, steps per second:   2, episode reward: 390.000, mean reward:  0.531 [ 0.000, 10.000], mean action: 3.580 [0.000, 8.000],  loss: 10.084927, mean_q: 19.824125, mean_eps: 0.100000\n",
      " 41837/50000: episode: 56, duration: 518.907s, episode steps: 1004, steps per second:   2, episode reward: 720.000, mean reward:  0.717 [ 0.000, 200.000], mean action: 4.616 [0.000, 8.000],  loss: 8.624207, mean_q: 19.904063, mean_eps: 0.100000\n",
      " 42418/50000: episode: 57, duration: 300.778s, episode steps: 581, steps per second:   2, episode reward: 280.000, mean reward:  0.482 [ 0.000, 10.000], mean action: 3.570 [0.000, 8.000],  loss: 4.644674, mean_q: 19.770069, mean_eps: 0.100000\n",
      " 43174/50000: episode: 58, duration: 391.744s, episode steps: 756, steps per second:   2, episode reward: 330.000, mean reward:  0.437 [ 0.000, 10.000], mean action: 4.332 [0.000, 8.000],  loss: 7.269570, mean_q: 19.912200, mean_eps: 0.100000\n",
      " 43841/50000: episode: 59, duration: 345.907s, episode steps: 667, steps per second:   2, episode reward: 180.000, mean reward:  0.270 [ 0.000, 10.000], mean action: 4.289 [0.000, 8.000],  loss: 3.062301, mean_q: 19.735721, mean_eps: 0.100000\n",
      " 44775/50000: episode: 60, duration: 488.836s, episode steps: 934, steps per second:   2, episode reward: 750.000, mean reward:  0.803 [ 0.000, 200.000], mean action: 4.279 [0.000, 8.000],  loss: 3.567841, mean_q: 19.757445, mean_eps: 0.100000\n",
      " 45468/50000: episode: 61, duration: 360.264s, episode steps: 693, steps per second:   2, episode reward: 240.000, mean reward:  0.346 [ 0.000, 10.000], mean action: 3.961 [0.000, 8.000],  loss: 3.508735, mean_q: 19.738723, mean_eps: 0.100000\n",
      " 46102/50000: episode: 62, duration: 329.611s, episode steps: 634, steps per second:   2, episode reward: 390.000, mean reward:  0.615 [ 0.000, 10.000], mean action: 4.380 [0.000, 8.000],  loss: 4.760936, mean_q: 19.795337, mean_eps: 0.100000\n",
      " 46965/50000: episode: 63, duration: 448.626s, episode steps: 863, steps per second:   2, episode reward: 730.000, mean reward:  0.846 [ 0.000, 200.000], mean action: 4.671 [0.000, 8.000],  loss: 2.977646, mean_q: 19.685593, mean_eps: 0.100000\n",
      " 47581/50000: episode: 64, duration: 326.074s, episode steps: 616, steps per second:   2, episode reward: 260.000, mean reward:  0.422 [ 0.000, 10.000], mean action: 3.912 [0.000, 8.000],  loss: 2.971104, mean_q: 19.742278, mean_eps: 0.100000\n",
      " 48110/50000: episode: 65, duration: 277.018s, episode steps: 529, steps per second:   2, episode reward: 210.000, mean reward:  0.397 [ 0.000, 10.000], mean action: 3.669 [0.000, 8.000],  loss: 3.212068, mean_q: 19.804464, mean_eps: 0.100000\n",
      " 48745/50000: episode: 66, duration: 331.167s, episode steps: 635, steps per second:   2, episode reward: 320.000, mean reward:  0.504 [ 0.000, 10.000], mean action: 4.066 [0.000, 8.000],  loss: 3.667664, mean_q: 19.707179, mean_eps: 0.100000\n",
      " 49622/50000: episode: 67, duration: 457.445s, episode steps: 877, steps per second:   2, episode reward: 480.000, mean reward:  0.547 [ 0.000, 50.000], mean action: 3.872 [0.000, 8.000],  loss: 2.226357, mean_q: 19.656118, mean_eps: 0.100000\n",
      "done, took 25206.587 seconds\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T07:30:27.705412Z",
     "iopub.status.busy": "2022-01-26T07:30:27.705412Z",
     "iopub.status.idle": "2022-01-26T07:31:46.689808Z",
     "shell.execute_reply": "2022-01-26T07:31:46.687806Z",
     "shell.execute_reply.started": "2022-01-26T07:30:27.705412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milod\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 500.000, steps: 731\n",
      "Episode 2: reward: 310.000, steps: 527\n",
      "Episode 3: reward: 180.000, steps: 598\n",
      "Episode 4: reward: 240.000, steps: 556\n",
      "Episode 5: reward: 1160.000, steps: 734\n",
      "Episode 6: reward: 490.000, steps: 934\n",
      "Episode 7: reward: 420.000, steps: 752\n",
      "Episode 8: reward: 400.000, steps: 713\n",
      "Episode 9: reward: 310.000, steps: 655\n",
      "Episode 10: reward: 340.000, steps: 639\n",
      "435.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights_v2/10k-Fast/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:01:07.442411Z",
     "iopub.status.busy": "2022-01-26T09:01:07.442411Z",
     "iopub.status.idle": "2022-01-26T09:01:07.457424Z",
     "shell.execute_reply": "2022-01-26T09:01:07.456423Z",
     "shell.execute_reply.started": "2022-01-26T09:01:07.442411Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39852/2342330577.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model, dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:28:40.790411Z",
     "iopub.status.busy": "2022-01-26T09:28:40.790411Z",
     "iopub.status.idle": "2022-01-26T09:28:40.995596Z",
     "shell.execute_reply": "2022-01-26T09:28:40.995596Z",
     "shell.execute_reply.started": "2022-01-26T09:28:40.790411Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html  \n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf  \n",
    "https://www.nature.com/articles/nature14236  \n",
    "### GPU\n",
    "### PreProces\n",
    "### Mniejsza sieć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:29:07.448211Z",
     "iopub.status.busy": "2022-01-26T09:29:07.448211Z",
     "iopub.status.idle": "2022-01-26T16:32:05.609388Z",
     "shell.execute_reply": "2022-01-26T16:32:05.608387Z",
     "shell.execute_reply.started": "2022-01-26T09:29:07.448211Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milod\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   565/50000: episode: 1, duration: 11.333s, episode steps: 565, steps per second:  50, episode reward: 130.000, mean reward:  0.230 [ 0.000, 10.000], mean action: 3.940 [0.000, 8.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milod\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1261/50000: episode: 2, duration: 138.460s, episode steps: 696, steps per second:   5, episode reward: 340.000, mean reward:  0.489 [ 0.000, 10.000], mean action: 4.066 [0.000, 8.000],  loss: 2.211012, mean_q: 20.425924, mean_eps: 0.898255\n",
      "  1856/50000: episode: 3, duration: 307.834s, episode steps: 595, steps per second:   2, episode reward: 220.000, mean reward:  0.370 [ 0.000, 10.000], mean action: 4.047 [0.000, 8.000],  loss: 0.875234, mean_q: 20.873092, mean_eps: 0.859780\n",
      "  2740/50000: episode: 4, duration: 456.046s, episode steps: 884, steps per second:   2, episode reward: 380.000, mean reward:  0.430 [ 0.000, 10.000], mean action: 3.896 [0.000, 8.000],  loss: 1.151486, mean_q: 20.930360, mean_eps: 0.793225\n",
      "  3395/50000: episode: 5, duration: 341.502s, episode steps: 655, steps per second:   2, episode reward: 290.000, mean reward:  0.443 [ 0.000, 10.000], mean action: 3.823 [0.000, 8.000],  loss: 0.873133, mean_q: 21.115546, mean_eps: 0.723970\n",
      "  4109/50000: episode: 6, duration: 367.405s, episode steps: 714, steps per second:   2, episode reward: 300.000, mean reward:  0.420 [ 0.000, 10.000], mean action: 4.057 [0.000, 8.000],  loss: 0.830719, mean_q: 21.035257, mean_eps: 0.662365\n",
      "  4846/50000: episode: 7, duration: 386.771s, episode steps: 737, steps per second:   2, episode reward: 330.000, mean reward:  0.448 [ 0.000, 10.000], mean action: 3.794 [0.000, 8.000],  loss: 0.741225, mean_q: 21.028040, mean_eps: 0.597070\n",
      "  5446/50000: episode: 8, duration: 311.321s, episode steps: 600, steps per second:   2, episode reward: 210.000, mean reward:  0.350 [ 0.000, 10.000], mean action: 3.600 [0.000, 8.000],  loss: 0.730027, mean_q: 20.946315, mean_eps: 0.536905\n",
      "  5981/50000: episode: 9, duration: 280.891s, episode steps: 535, steps per second:   2, episode reward: 240.000, mean reward:  0.449 [ 0.000, 10.000], mean action: 3.523 [0.000, 8.000],  loss: 0.729155, mean_q: 20.928234, mean_eps: 0.485830\n",
      "  6703/50000: episode: 10, duration: 375.247s, episode steps: 722, steps per second:   2, episode reward: 400.000, mean reward:  0.554 [ 0.000, 10.000], mean action: 3.965 [0.000, 8.000],  loss: 0.740807, mean_q: 20.721140, mean_eps: 0.429265\n",
      "  7237/50000: episode: 11, duration: 277.374s, episode steps: 534, steps per second:   2, episode reward: 270.000, mean reward:  0.506 [ 0.000, 10.000], mean action: 3.361 [0.000, 8.000],  loss: 0.811178, mean_q: 20.749369, mean_eps: 0.372745\n",
      "  7841/50000: episode: 12, duration: 317.232s, episode steps: 604, steps per second:   2, episode reward: 240.000, mean reward:  0.397 [ 0.000, 10.000], mean action: 3.548 [0.000, 8.000],  loss: 0.839047, mean_q: 20.748807, mean_eps: 0.321535\n",
      "  8583/50000: episode: 13, duration: 386.849s, episode steps: 742, steps per second:   2, episode reward: 370.000, mean reward:  0.499 [ 0.000, 10.000], mean action: 3.016 [0.000, 8.000],  loss: 0.757180, mean_q: 20.749583, mean_eps: 0.260965\n",
      "  9171/50000: episode: 14, duration: 304.551s, episode steps: 588, steps per second:   2, episode reward: 240.000, mean reward:  0.408 [ 0.000, 10.000], mean action: 3.364 [0.000, 8.000],  loss: 0.773956, mean_q: 20.787063, mean_eps: 0.201115\n",
      "  9786/50000: episode: 15, duration: 314.311s, episode steps: 615, steps per second:   2, episode reward: 340.000, mean reward:  0.553 [ 0.000, 10.000], mean action: 3.081 [0.000, 8.000],  loss: 0.824452, mean_q: 20.785596, mean_eps: 0.146980\n",
      " 10644/50000: episode: 16, duration: 438.404s, episode steps: 858, steps per second:   2, episode reward: 360.000, mean reward:  0.420 [ 0.000, 10.000], mean action: 3.301 [0.000, 8.000],  loss: 2.687862, mean_q: 21.764115, mean_eps: 0.102413\n",
      " 11211/50000: episode: 17, duration: 289.705s, episode steps: 567, steps per second:   2, episode reward: 370.000, mean reward:  0.653 [ 0.000, 10.000], mean action: 3.698 [0.000, 8.000],  loss: 1.710200, mean_q: 21.983973, mean_eps: 0.100000\n",
      " 11866/50000: episode: 18, duration: 336.356s, episode steps: 655, steps per second:   2, episode reward: 410.000, mean reward:  0.626 [ 0.000, 10.000], mean action: 2.948 [0.000, 8.000],  loss: 1.237722, mean_q: 22.099350, mean_eps: 0.100000\n",
      " 12714/50000: episode: 19, duration: 434.525s, episode steps: 848, steps per second:   2, episode reward: 620.000, mean reward:  0.731 [ 0.000, 50.000], mean action: 3.295 [0.000, 8.000],  loss: 1.088478, mean_q: 22.117912, mean_eps: 0.100000\n",
      " 13433/50000: episode: 20, duration: 368.579s, episode steps: 719, steps per second:   2, episode reward: 330.000, mean reward:  0.459 [ 0.000, 10.000], mean action: 3.245 [0.000, 8.000],  loss: 1.017190, mean_q: 22.059770, mean_eps: 0.100000\n",
      " 14232/50000: episode: 21, duration: 410.679s, episode steps: 799, steps per second:   2, episode reward: 600.000, mean reward:  0.751 [ 0.000, 50.000], mean action: 3.176 [0.000, 8.000],  loss: 0.977589, mean_q: 22.027182, mean_eps: 0.100000\n",
      " 15225/50000: episode: 22, duration: 507.663s, episode steps: 993, steps per second:   2, episode reward: 360.000, mean reward:  0.363 [ 0.000, 50.000], mean action: 3.060 [0.000, 8.000],  loss: 1.001217, mean_q: 22.116251, mean_eps: 0.100000\n",
      " 16004/50000: episode: 23, duration: 398.644s, episode steps: 779, steps per second:   2, episode reward: 540.000, mean reward:  0.693 [ 0.000, 200.000], mean action: 3.520 [0.000, 8.000],  loss: 2.614018, mean_q: 22.233706, mean_eps: 0.100000\n",
      " 16661/50000: episode: 24, duration: 336.288s, episode steps: 657, steps per second:   2, episode reward: 410.000, mean reward:  0.624 [ 0.000, 10.000], mean action: 3.452 [0.000, 8.000],  loss: 2.449572, mean_q: 22.341673, mean_eps: 0.100000\n",
      " 17972/50000: episode: 25, duration: 670.871s, episode steps: 1311, steps per second:   2, episode reward: 1330.000, mean reward:  1.014 [ 0.000, 200.000], mean action: 3.031 [0.000, 8.000],  loss: 2.985630, mean_q: 22.261760, mean_eps: 0.100000\n",
      " 18702/50000: episode: 26, duration: 373.246s, episode steps: 730, steps per second:   2, episode reward: 350.000, mean reward:  0.479 [ 0.000, 10.000], mean action: 3.064 [0.000, 8.000],  loss: 4.017494, mean_q: 22.340694, mean_eps: 0.100000\n",
      " 19542/50000: episode: 27, duration: 429.206s, episode steps: 840, steps per second:   2, episode reward: 320.000, mean reward:  0.381 [ 0.000, 50.000], mean action: 3.492 [0.000, 8.000],  loss: 3.153903, mean_q: 22.359291, mean_eps: 0.100000\n",
      " 19963/50000: episode: 28, duration: 215.361s, episode steps: 421, steps per second:   2, episode reward: 150.000, mean reward:  0.356 [ 0.000, 10.000], mean action: 3.126 [0.000, 8.000],  loss: 1.796236, mean_q: 22.256776, mean_eps: 0.100000\n",
      " 20581/50000: episode: 29, duration: 316.090s, episode steps: 618, steps per second:   2, episode reward: 390.000, mean reward:  0.631 [ 0.000, 50.000], mean action: 3.660 [0.000, 8.000],  loss: 4.876166, mean_q: 23.348696, mean_eps: 0.100000\n",
      " 21498/50000: episode: 30, duration: 469.613s, episode steps: 917, steps per second:   2, episode reward: 720.000, mean reward:  0.785 [ 0.000, 50.000], mean action: 3.766 [0.000, 8.000],  loss: 3.730794, mean_q: 23.543787, mean_eps: 0.100000\n",
      " 22200/50000: episode: 31, duration: 359.336s, episode steps: 702, steps per second:   2, episode reward: 470.000, mean reward:  0.670 [ 0.000, 10.000], mean action: 3.211 [0.000, 8.000],  loss: 2.513169, mean_q: 23.442074, mean_eps: 0.100000\n",
      " 22718/50000: episode: 32, duration: 265.268s, episode steps: 518, steps per second:   2, episode reward: 240.000, mean reward:  0.463 [ 0.000, 10.000], mean action: 3.303 [0.000, 8.000],  loss: 1.953596, mean_q: 23.472623, mean_eps: 0.100000\n",
      " 23357/50000: episode: 33, duration: 327.099s, episode steps: 639, steps per second:   2, episode reward: 230.000, mean reward:  0.360 [ 0.000, 10.000], mean action: 3.080 [0.000, 8.000],  loss: 1.691288, mean_q: 23.405154, mean_eps: 0.100000\n",
      " 24318/50000: episode: 34, duration: 492.070s, episode steps: 961, steps per second:   2, episode reward: 1150.000, mean reward:  1.197 [ 0.000, 400.000], mean action: 3.626 [0.000, 8.000],  loss: 1.451629, mean_q: 23.439588, mean_eps: 0.100000\n",
      " 24876/50000: episode: 35, duration: 286.352s, episode steps: 558, steps per second:   2, episode reward: 230.000, mean reward:  0.412 [ 0.000, 10.000], mean action: 4.403 [0.000, 8.000],  loss: 1.494602, mean_q: 23.377941, mean_eps: 0.100000\n",
      " 25467/50000: episode: 36, duration: 303.223s, episode steps: 591, steps per second:   2, episode reward: 370.000, mean reward:  0.626 [ 0.000, 10.000], mean action: 3.369 [0.000, 8.000],  loss: 9.366370, mean_q: 23.491202, mean_eps: 0.100000\n",
      " 26119/50000: episode: 37, duration: 334.212s, episode steps: 652, steps per second:   2, episode reward: 260.000, mean reward:  0.399 [ 0.000, 10.000], mean action: 3.489 [0.000, 8.000],  loss: 3.520096, mean_q: 23.371569, mean_eps: 0.100000\n",
      " 26686/50000: episode: 38, duration: 291.256s, episode steps: 567, steps per second:   2, episode reward: 270.000, mean reward:  0.476 [ 0.000, 10.000], mean action: 3.349 [0.000, 8.000],  loss: 1.332083, mean_q: 23.299782, mean_eps: 0.100000\n",
      " 27420/50000: episode: 39, duration: 377.409s, episode steps: 734, steps per second:   2, episode reward: 530.000, mean reward:  0.722 [ 0.000, 10.000], mean action: 3.501 [0.000, 8.000],  loss: 1.881227, mean_q: 23.250043, mean_eps: 0.100000\n",
      " 28252/50000: episode: 40, duration: 427.768s, episode steps: 832, steps per second:   2, episode reward: 430.000, mean reward:  0.517 [ 0.000, 10.000], mean action: 3.282 [0.000, 8.000],  loss: 3.364288, mean_q: 23.224657, mean_eps: 0.100000\n",
      " 29101/50000: episode: 41, duration: 436.871s, episode steps: 849, steps per second:   2, episode reward: 1100.000, mean reward:  1.296 [ 0.000, 400.000], mean action: 3.269 [0.000, 8.000],  loss: 4.867722, mean_q: 23.296714, mean_eps: 0.100000\n",
      " 29489/50000: episode: 42, duration: 200.013s, episode steps: 388, steps per second:   2, episode reward: 150.000, mean reward:  0.387 [ 0.000, 10.000], mean action: 3.054 [0.000, 8.000],  loss: 12.046052, mean_q: 23.550413, mean_eps: 0.100000\n",
      " 30327/50000: episode: 43, duration: 431.372s, episode steps: 838, steps per second:   2, episode reward: 550.000, mean reward:  0.656 [ 0.000, 50.000], mean action: 3.632 [0.000, 8.000],  loss: 4.104103, mean_q: 23.638874, mean_eps: 0.100000\n",
      " 31028/50000: episode: 44, duration: 361.611s, episode steps: 701, steps per second:   2, episode reward: 310.000, mean reward:  0.442 [ 0.000, 10.000], mean action: 3.218 [0.000, 8.000],  loss: 6.607434, mean_q: 24.373722, mean_eps: 0.100000\n",
      " 31610/50000: episode: 45, duration: 300.214s, episode steps: 582, steps per second:   2, episode reward: 320.000, mean reward:  0.550 [ 0.000, 10.000], mean action: 3.476 [0.000, 8.000],  loss: 15.783280, mean_q: 24.638028, mean_eps: 0.100000\n",
      " 32691/50000: episode: 46, duration: 556.821s, episode steps: 1081, steps per second:   2, episode reward: 970.000, mean reward:  0.897 [ 0.000, 100.000], mean action: 3.865 [0.000, 8.000],  loss: 4.391562, mean_q: 24.505578, mean_eps: 0.100000\n",
      " 33529/50000: episode: 47, duration: 439.720s, episode steps: 838, steps per second:   2, episode reward: 1340.000, mean reward:  1.599 [ 0.000, 400.000], mean action: 4.264 [0.000, 8.000],  loss: 6.305519, mean_q: 24.476642, mean_eps: 0.100000\n",
      " 34248/50000: episode: 48, duration: 371.044s, episode steps: 719, steps per second:   2, episode reward: 500.000, mean reward:  0.695 [ 0.000, 10.000], mean action: 3.825 [0.000, 8.000],  loss: 9.026016, mean_q: 24.674318, mean_eps: 0.100000\n",
      " 35333/50000: episode: 49, duration: 560.671s, episode steps: 1085, steps per second:   2, episode reward: 870.000, mean reward:  0.802 [ 0.000, 200.000], mean action: 4.205 [0.000, 8.000],  loss: 6.403714, mean_q: 24.647633, mean_eps: 0.100000\n",
      " 36021/50000: episode: 50, duration: 356.325s, episode steps: 688, steps per second:   2, episode reward: 310.000, mean reward:  0.451 [ 0.000, 10.000], mean action: 3.767 [0.000, 8.000],  loss: 5.490874, mean_q: 24.532968, mean_eps: 0.100000\n",
      " 36660/50000: episode: 51, duration: 330.727s, episode steps: 639, steps per second:   2, episode reward: 310.000, mean reward:  0.485 [ 0.000, 10.000], mean action: 3.905 [0.000, 8.000],  loss: 6.568740, mean_q: 24.696672, mean_eps: 0.100000\n",
      " 37384/50000: episode: 52, duration: 383.393s, episode steps: 724, steps per second:   2, episode reward: 500.000, mean reward:  0.691 [ 0.000, 50.000], mean action: 4.552 [0.000, 8.000],  loss: 8.714012, mean_q: 25.118369, mean_eps: 0.100000\n",
      " 37909/50000: episode: 53, duration: 277.746s, episode steps: 525, steps per second:   2, episode reward: 290.000, mean reward:  0.552 [ 0.000, 10.000], mean action: 4.112 [0.000, 8.000],  loss: 3.241083, mean_q: 24.546629, mean_eps: 0.100000\n",
      " 38724/50000: episode: 54, duration: 431.656s, episode steps: 815, steps per second:   2, episode reward: 680.000, mean reward:  0.834 [ 0.000, 200.000], mean action: 4.361 [0.000, 8.000],  loss: 5.552126, mean_q: 24.799131, mean_eps: 0.100000\n",
      " 39424/50000: episode: 55, duration: 368.689s, episode steps: 700, steps per second:   2, episode reward: 560.000, mean reward:  0.800 [ 0.000, 200.000], mean action: 4.626 [0.000, 8.000],  loss: 3.898327, mean_q: 24.721982, mean_eps: 0.100000\n",
      " 40242/50000: episode: 56, duration: 425.243s, episode steps: 818, steps per second:   2, episode reward: 580.000, mean reward:  0.709 [ 0.000, 50.000], mean action: 4.676 [0.000, 8.000],  loss: 5.702972, mean_q: 25.097685, mean_eps: 0.100000\n",
      " 40817/50000: episode: 57, duration: 306.054s, episode steps: 575, steps per second:   2, episode reward: 230.000, mean reward:  0.400 [ 0.000, 10.000], mean action: 3.697 [0.000, 8.000],  loss: 23.395517, mean_q: 26.311270, mean_eps: 0.100000\n",
      " 41502/50000: episode: 58, duration: 364.189s, episode steps: 685, steps per second:   2, episode reward: 760.000, mean reward:  1.109 [ 0.000, 200.000], mean action: 3.399 [0.000, 8.000],  loss: 12.015128, mean_q: 26.195092, mean_eps: 0.100000\n",
      " 42309/50000: episode: 59, duration: 424.657s, episode steps: 807, steps per second:   2, episode reward: 1110.000, mean reward:  1.375 [ 0.000, 400.000], mean action: 4.344 [0.000, 8.000],  loss: 8.577927, mean_q: 26.154286, mean_eps: 0.100000\n",
      " 42943/50000: episode: 60, duration: 332.941s, episode steps: 634, steps per second:   2, episode reward: 310.000, mean reward:  0.489 [ 0.000, 10.000], mean action: 3.631 [0.000, 8.000],  loss: 16.496920, mean_q: 26.075197, mean_eps: 0.100000\n",
      " 43680/50000: episode: 61, duration: 383.758s, episode steps: 737, steps per second:   2, episode reward: 350.000, mean reward:  0.475 [ 0.000, 10.000], mean action: 4.275 [0.000, 8.000],  loss: 10.984750, mean_q: 26.431407, mean_eps: 0.100000\n",
      " 44415/50000: episode: 62, duration: 382.336s, episode steps: 735, steps per second:   2, episode reward: 440.000, mean reward:  0.599 [ 0.000, 10.000], mean action: 3.999 [0.000, 8.000],  loss: 11.403787, mean_q: 26.239165, mean_eps: 0.100000\n",
      " 45021/50000: episode: 63, duration: 315.299s, episode steps: 606, steps per second:   2, episode reward: 310.000, mean reward:  0.512 [ 0.000, 10.000], mean action: 4.482 [0.000, 8.000],  loss: 12.636545, mean_q: 26.406495, mean_eps: 0.100000\n",
      " 45766/50000: episode: 64, duration: 387.959s, episode steps: 745, steps per second:   2, episode reward: 470.000, mean reward:  0.631 [ 0.000, 50.000], mean action: 3.389 [0.000, 8.000],  loss: 7.165788, mean_q: 26.400934, mean_eps: 0.100000\n",
      " 46419/50000: episode: 65, duration: 339.885s, episode steps: 653, steps per second:   2, episode reward: 380.000, mean reward:  0.582 [ 0.000, 10.000], mean action: 4.478 [0.000, 8.000],  loss: 6.092901, mean_q: 26.411476, mean_eps: 0.100000\n",
      " 47103/50000: episode: 66, duration: 356.409s, episode steps: 684, steps per second:   2, episode reward: 400.000, mean reward:  0.585 [ 0.000, 10.000], mean action: 3.905 [0.000, 8.000],  loss: 6.244829, mean_q: 26.410171, mean_eps: 0.100000\n",
      " 47810/50000: episode: 67, duration: 368.801s, episode steps: 707, steps per second:   2, episode reward: 310.000, mean reward:  0.438 [ 0.000, 10.000], mean action: 4.083 [0.000, 8.000],  loss: 5.214072, mean_q: 26.168152, mean_eps: 0.100000\n",
      " 48603/50000: episode: 68, duration: 413.384s, episode steps: 793, steps per second:   2, episode reward: 640.000, mean reward:  0.807 [ 0.000, 110.000], mean action: 4.683 [0.000, 8.000],  loss: 5.777901, mean_q: 26.151872, mean_eps: 0.100000\n",
      " 49231/50000: episode: 69, duration: 331.832s, episode steps: 628, steps per second:   2, episode reward: 280.000, mean reward:  0.446 [ 0.000, 10.000], mean action: 3.982 [0.000, 8.000],  loss: 3.847751, mean_q: 26.128131, mean_eps: 0.100000\n",
      " 49921/50000: episode: 70, duration: 360.580s, episode steps: 690, steps per second:   2, episode reward: 370.000, mean reward:  0.536 [ 0.000, 10.000], mean action: 4.799 [0.000, 8.000],  loss: 4.760137, mean_q: 26.026687, mean_eps: 0.100000\n",
      "done, took 25377.871 seconds\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "dqn.save_weights('SavedWeights_v2/10k-Fast/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T18:21:35.802723Z",
     "iopub.status.busy": "2022-01-26T18:21:35.802723Z",
     "iopub.status.idle": "2022-01-26T18:21:37.224923Z",
     "shell.execute_reply": "2022-01-26T18:21:37.223922Z",
     "shell.execute_reply.started": "2022-01-26T18:21:35.802723Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Pacman_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Pacman_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T18:26:19.599300Z",
     "iopub.status.busy": "2022-01-26T18:26:19.599300Z",
     "iopub.status.idle": "2022-01-26T18:28:37.637404Z",
     "shell.execute_reply": "2022-01-26T18:28:37.636403Z",
     "shell.execute_reply.started": "2022-01-26T18:26:19.599300Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 990.000, steps: 796\n",
      "Episode 2: reward: 160.000, steps: 453\n",
      "Episode 3: reward: 430.000, steps: 819\n",
      "Episode 4: reward: 370.000, steps: 622\n",
      "Episode 5: reward: 880.000, steps: 955\n",
      "Episode 6: reward: 370.000, steps: 1042\n",
      "Episode 7: reward: 470.000, steps: 1017\n",
      "Episode 8: reward: 460.000, steps: 844\n",
      "Episode 9: reward: 1240.000, steps: 1001\n",
      "Episode 10: reward: 510.000, steps: 704\n",
      "588.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "361823495b17489efeaeccf318091240e132297ba5d4dac12e93ed57064eb06e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
